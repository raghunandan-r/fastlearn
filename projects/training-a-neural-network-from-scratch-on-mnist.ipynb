{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":3004,"databundleVersionId":861823,"sourceType":"competition"}],"dockerImageVersionId":30527,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"### Introduction\n\nI work on small projects as part of my progress in completing the fastai v2 course. I use kaggle to implement whatever I've learnt after each chapter [right now at Chapter 4!](https://github.com/fastai/fastbook/blob/master/04_mnist_basics.ipynb). \n\nIn the chapter, a neural network was developed from scratch to distingush between 3 and 7 from the MNIST dataset. In this notebook, we change the code to classify the whole dataset (0-9).\n","metadata":{}},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session\n\n!pip install --upgrade fastai\nfrom fastai.vision.all import *\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-01-01T22:34:55.787165Z","iopub.execute_input":"2024-01-01T22:34:55.787637Z","iopub.status.idle":"2024-01-01T22:35:15.053699Z","shell.execute_reply.started":"2024-01-01T22:34:55.787601Z","shell.execute_reply":"2024-01-01T22:35:15.052528Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"/kaggle/input/digit-recognizer/sample_submission.csv\n/kaggle/input/digit-recognizer/train.csv\n/kaggle/input/digit-recognizer/test.csv\nRequirement already satisfied: fastai in /opt/conda/lib/python3.10/site-packages (2.7.12)\nCollecting fastai\n  Downloading fastai-2.7.13-py3-none-any.whl (232 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m232.2/232.2 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hRequirement already satisfied: pip in /opt/conda/lib/python3.10/site-packages (from fastai) (23.1.2)\nRequirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from fastai) (21.3)\nRequirement already satisfied: fastdownload<2,>=0.0.5 in /opt/conda/lib/python3.10/site-packages (from fastai) (0.0.7)\nRequirement already satisfied: fastcore<1.6,>=1.5.29 in /opt/conda/lib/python3.10/site-packages (from fastai) (1.5.29)\nRequirement already satisfied: torchvision>=0.11 in /opt/conda/lib/python3.10/site-packages (from fastai) (0.15.1+cpu)\nRequirement already satisfied: matplotlib in /opt/conda/lib/python3.10/site-packages (from fastai) (3.7.1)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from fastai) (1.5.3)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from fastai) (2.31.0)\nRequirement already satisfied: pyyaml in /opt/conda/lib/python3.10/site-packages (from fastai) (6.0)\nRequirement already satisfied: fastprogress>=0.2.4 in /opt/conda/lib/python3.10/site-packages (from fastai) (1.0.3)\nRequirement already satisfied: pillow>=9.0.0 in /opt/conda/lib/python3.10/site-packages (from fastai) (9.5.0)\nRequirement already satisfied: scikit-learn in /opt/conda/lib/python3.10/site-packages (from fastai) (1.2.2)\nRequirement already satisfied: scipy in /opt/conda/lib/python3.10/site-packages (from fastai) (1.11.1)\nRequirement already satisfied: spacy<4 in /opt/conda/lib/python3.10/site-packages (from fastai) (3.6.0)\nRequirement already satisfied: torch<2.2,>=1.10 in /opt/conda/lib/python3.10/site-packages (from fastai) (2.0.0+cpu)\nRequirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /opt/conda/lib/python3.10/site-packages (from spacy<4->fastai) (3.0.12)\nRequirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from spacy<4->fastai) (1.0.4)\nRequirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /opt/conda/lib/python3.10/site-packages (from spacy<4->fastai) (1.0.9)\nRequirement already satisfied: cymem<2.1.0,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from spacy<4->fastai) (2.0.7)\nRequirement already satisfied: preshed<3.1.0,>=3.0.2 in /opt/conda/lib/python3.10/site-packages (from spacy<4->fastai) (3.0.8)\nRequirement already satisfied: thinc<8.2.0,>=8.1.8 in /opt/conda/lib/python3.10/site-packages (from spacy<4->fastai) (8.1.10)\nRequirement already satisfied: wasabi<1.2.0,>=0.9.1 in /opt/conda/lib/python3.10/site-packages (from spacy<4->fastai) (1.1.2)\nRequirement already satisfied: srsly<3.0.0,>=2.4.3 in /opt/conda/lib/python3.10/site-packages (from spacy<4->fastai) (2.4.6)\nRequirement already satisfied: catalogue<2.1.0,>=2.0.6 in /opt/conda/lib/python3.10/site-packages (from spacy<4->fastai) (2.0.8)\nRequirement already satisfied: typer<0.10.0,>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from spacy<4->fastai) (0.9.0)\nRequirement already satisfied: pathy>=0.10.0 in /opt/conda/lib/python3.10/site-packages (from spacy<4->fastai) (0.10.2)\nRequirement already satisfied: smart-open<7.0.0,>=5.2.1 in /opt/conda/lib/python3.10/site-packages (from spacy<4->fastai) (6.3.0)\nRequirement already satisfied: tqdm<5.0.0,>=4.38.0 in /opt/conda/lib/python3.10/site-packages (from spacy<4->fastai) (4.65.0)\nRequirement already satisfied: numpy>=1.15.0 in /opt/conda/lib/python3.10/site-packages (from spacy<4->fastai) (1.23.5)\nRequirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in /opt/conda/lib/python3.10/site-packages (from spacy<4->fastai) (1.10.9)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from spacy<4->fastai) (3.1.2)\nRequirement already satisfied: setuptools in /opt/conda/lib/python3.10/site-packages (from spacy<4->fastai) (59.8.0)\nRequirement already satisfied: langcodes<4.0.0,>=3.2.0 in /opt/conda/lib/python3.10/site-packages (from spacy<4->fastai) (3.3.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging->fastai) (3.0.9)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->fastai) (3.1.0)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->fastai) (3.4)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->fastai) (1.26.15)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->fastai) (2023.5.7)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch<2.2,>=1.10->fastai) (3.12.2)\nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch<2.2,>=1.10->fastai) (4.6.3)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch<2.2,>=1.10->fastai) (1.12)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch<2.2,>=1.10->fastai) (3.1)\nRequirement already satisfied: contourpy>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib->fastai) (1.1.0)\nRequirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.10/site-packages (from matplotlib->fastai) (0.11.0)\nRequirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib->fastai) (4.40.0)\nRequirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib->fastai) (1.4.4)\nRequirement already satisfied: python-dateutil>=2.7 in /opt/conda/lib/python3.10/site-packages (from matplotlib->fastai) (2.8.2)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->fastai) (2023.3)\nRequirement already satisfied: joblib>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from scikit-learn->fastai) (1.2.0)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn->fastai) (3.1.0)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.7->matplotlib->fastai) (1.16.0)\nRequirement already satisfied: blis<0.8.0,>=0.7.8 in /opt/conda/lib/python3.10/site-packages (from thinc<8.2.0,>=8.1.8->spacy<4->fastai) (0.7.9)\nRequirement already satisfied: confection<1.0.0,>=0.0.1 in /opt/conda/lib/python3.10/site-packages (from thinc<8.2.0,>=8.1.8->spacy<4->fastai) (0.1.0)\nRequirement already satisfied: click<9.0.0,>=7.1.1 in /opt/conda/lib/python3.10/site-packages (from typer<0.10.0,>=0.3.0->spacy<4->fastai) (8.1.3)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->spacy<4->fastai) (2.1.3)\nRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch<2.2,>=1.10->fastai) (1.3.0)\nInstalling collected packages: fastai\n  Attempting uninstall: fastai\n    Found existing installation: fastai 2.7.12\n    Uninstalling fastai-2.7.12:\n      Successfully uninstalled fastai-2.7.12\nSuccessfully installed fastai-2.7.13\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n","output_type":"stream"}]},{"cell_type":"markdown","source":"I setup a path pointing to the data first and also created a folder to store the images that will be created from the data. Note that the data provided is a csv file and not images.","metadata":{}},{"cell_type":"code","source":"path = Path(\"../input/digit-recognizer\")\npath.ls()\nimages_path = Path(\"../images\")\nimages_path.mkdir(exist_ok=True)\n\ntrain = pd.read_csv(path/\"train.csv\")\ntest = pd.read_csv(path/\"test.csv\")","metadata":{"execution":{"iopub.status.busy":"2024-01-01T22:35:48.674461Z","iopub.execute_input":"2024-01-01T22:35:48.674909Z","iopub.status.idle":"2024-01-01T22:35:54.611153Z","shell.execute_reply.started":"2024-01-01T22:35:48.674873Z","shell.execute_reply":"2024-01-01T22:35:54.610160Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"train.head()","metadata":{"execution":{"iopub.status.busy":"2023-07-23T04:10:06.590287Z","iopub.execute_input":"2023-07-23T04:10:06.590756Z","iopub.status.idle":"2023-07-23T04:10:06.619605Z","shell.execute_reply.started":"2023-07-23T04:10:06.590715Z","shell.execute_reply":"2023-07-23T04:10:06.618285Z"},"trusted":true},"execution_count":6,"outputs":[{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"   label  pixel0  pixel1  pixel2  pixel3  pixel4  pixel5  pixel6  pixel7  \\\n0      1       0       0       0       0       0       0       0       0   \n1      0       0       0       0       0       0       0       0       0   \n2      1       0       0       0       0       0       0       0       0   \n3      4       0       0       0       0       0       0       0       0   \n4      0       0       0       0       0       0       0       0       0   \n\n   pixel8  ...  pixel774  pixel775  pixel776  pixel777  pixel778  pixel779  \\\n0       0  ...         0         0         0         0         0         0   \n1       0  ...         0         0         0         0         0         0   \n2       0  ...         0         0         0         0         0         0   \n3       0  ...         0         0         0         0         0         0   \n4       0  ...         0         0         0         0         0         0   \n\n   pixel780  pixel781  pixel782  pixel783  \n0         0         0         0         0  \n1         0         0         0         0  \n2         0         0         0         0  \n3         0         0         0         0  \n4         0         0         0         0  \n\n[5 rows x 785 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>label</th>\n      <th>pixel0</th>\n      <th>pixel1</th>\n      <th>pixel2</th>\n      <th>pixel3</th>\n      <th>pixel4</th>\n      <th>pixel5</th>\n      <th>pixel6</th>\n      <th>pixel7</th>\n      <th>pixel8</th>\n      <th>...</th>\n      <th>pixel774</th>\n      <th>pixel775</th>\n      <th>pixel776</th>\n      <th>pixel777</th>\n      <th>pixel778</th>\n      <th>pixel779</th>\n      <th>pixel780</th>\n      <th>pixel781</th>\n      <th>pixel782</th>\n      <th>pixel783</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>4</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows × 785 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"test.head()","metadata":{"execution":{"iopub.status.busy":"2023-12-31T00:04:05.683844Z","iopub.execute_input":"2023-12-31T00:04:05.685277Z","iopub.status.idle":"2023-12-31T00:04:05.718262Z","shell.execute_reply.started":"2023-12-31T00:04:05.685219Z","shell.execute_reply":"2023-12-31T00:04:05.716902Z"},"trusted":true},"execution_count":5,"outputs":[{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"   pixel0  pixel1  pixel2  pixel3  pixel4  pixel5  pixel6  pixel7  pixel8  \\\n0       0       0       0       0       0       0       0       0       0   \n1       0       0       0       0       0       0       0       0       0   \n2       0       0       0       0       0       0       0       0       0   \n3       0       0       0       0       0       0       0       0       0   \n4       0       0       0       0       0       0       0       0       0   \n\n   pixel9  ...  pixel774  pixel775  pixel776  pixel777  pixel778  pixel779  \\\n0       0  ...         0         0         0         0         0         0   \n1       0  ...         0         0         0         0         0         0   \n2       0  ...         0         0         0         0         0         0   \n3       0  ...         0         0         0         0         0         0   \n4       0  ...         0         0         0         0         0         0   \n\n   pixel780  pixel781  pixel782  pixel783  \n0         0         0         0         0  \n1         0         0         0         0  \n2         0         0         0         0  \n3         0         0         0         0  \n4         0         0         0         0  \n\n[5 rows x 784 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>pixel0</th>\n      <th>pixel1</th>\n      <th>pixel2</th>\n      <th>pixel3</th>\n      <th>pixel4</th>\n      <th>pixel5</th>\n      <th>pixel6</th>\n      <th>pixel7</th>\n      <th>pixel8</th>\n      <th>pixel9</th>\n      <th>...</th>\n      <th>pixel774</th>\n      <th>pixel775</th>\n      <th>pixel776</th>\n      <th>pixel777</th>\n      <th>pixel778</th>\n      <th>pixel779</th>\n      <th>pixel780</th>\n      <th>pixel781</th>\n      <th>pixel782</th>\n      <th>pixel783</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows × 784 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","source":"Other solutions convert the train dataset to images and work with [datablocks](https://docs.fast.ai/data.block.html#datablock), but as I'm trying to implement solutions in Ch-4, I'm going ahead with tensors and labels as X & Y. Note that the data in train_x are numbers in the range from 0 to 255, here it is one matrix instead of 3 usually in RGB. Hence it is best to normalize it (divide all by 255).","metadata":{}},{"cell_type":"code","source":"train_x = tensor(train.iloc[:,1:]).float()/255\ntrain_y = tensor(train.iloc[:,:1])\ntrain_x.shape, train_y.shape","metadata":{"execution":{"iopub.status.busy":"2024-01-01T22:36:01.586073Z","iopub.execute_input":"2024-01-01T22:36:01.586579Z","iopub.status.idle":"2024-01-01T22:36:01.868706Z","shell.execute_reply.started":"2024-01-01T22:36:01.586532Z","shell.execute_reply":"2024-01-01T22:36:01.867852Z"},"trusted":true},"execution_count":3,"outputs":[{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"(torch.Size([42000, 784]), torch.Size([42000, 1]))"},"metadata":{}}]},{"cell_type":"markdown","source":"Good idea to check if the rows indexed as train_x correspond to the labels by using fastai's *show_image*. `view` is a pytorch method to change the shape of a tensor without changing its contents. ","metadata":{}},{"cell_type":"code","source":"show_image(train_x[0].view(-1,28))","metadata":{"execution":{"iopub.status.busy":"2024-01-01T22:36:05.420369Z","iopub.execute_input":"2024-01-01T22:36:05.420799Z","iopub.status.idle":"2024-01-01T22:36:05.524171Z","shell.execute_reply.started":"2024-01-01T22:36:05.420763Z","shell.execute_reply":"2024-01-01T22:36:05.522661Z"},"trusted":true},"execution_count":4,"outputs":[{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"<Axes: >"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<Figure size 100x100 with 1 Axes>","image/png":"iVBORw0KGgoAAAANSUhEUgAAAGEAAABhCAYAAADGBs+jAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8pXeV/AAAACXBIWXMAAA9hAAAPYQGoP6dpAAALyklEQVR4nO2cSXNc13WAv3vf2POAGSAmDiJN2jJNkZLpiiNbFWXwwo5dyS4LL7JJFvkR2eYHZJ+KK1XxwpWUU8pUciqq2JFEc1I4IAJBgCAgEI1u9Nz9pnuzAEdZtlgKG3gNvq8KG9RdHODrc949597XQmutSThQ5EEHkJBIiAWJhBiQSIgBiYQYkEiIAYmEGJBIiAGJhBhgPu/Ct+UfDzKOQ8m/qR8/17okE2JAIiEGJBJiQCIhBiQSYkAiIQYkEmJAIiEGJBJiwHN3zIcVI59HL0wTZRyUYxCmDADcnT5GpQFBiNqto7rdgcXw0kvQC9Pc/aMS/rxHodjl7MQGEs27l08z9n4Wp6nIfeTA8t2BxfBySxCCKOvgz3t8/fhdLhZX+JP8LQC+1xhjd3UKZUqyrjPQMF5KCTKTITp7gs6MS+Oo5JXZNc4X1jjhbGEJSV9HKC1gny4DvZwSyiXW3kqTeWOHU4Uafzr1HqftKmkhsIRNV0eP//9iH0S8XBKkgbBMdNrFKykujm6ymN5h1qwzKm0Uir4O6SiNH5rIEEQEQqmBhvXySJAGxqljdBcLtCdNskfrvFm8zaTZoCwjDGGyHMDPu8e5641RWy4zuxRg133EbnOgob00EoRl0l0ssPW6hT8a8f3ZJd5Or+IKSVo6SCQf+yP844Ovcr9RIL8syVxfR7fbRJ3eQGM79BKEaSIcB5HN0C8Z+CMRsuQx6TRIy72eoKsCIjRr/ihrtRLdWprxpkb3+2g/AJ2Uo/8XxtgowdFJ/KJF5TX4zhtXmXVrvJW5hYVBQ/ksBXmqUZYf3T2P+695ytuK7Eod3e6gwxAdRQON8dBL0Pks7SMu/ZIgu1jnL8bfZUwKHGFiCYMAuBeUueePUtsocur9BmJ9C93povr9fYnxcEoQAmHbCMMgHMnQnpF4Zc1ivokrNFJIAiICHbEZpnivcZKV1gj2joHo+Wg/QEeDLUFPcyglCNvGKBXBddg5mUb9VoMT5RrfnbhGQRpYGNSUT0tJ3m2f5t8vnyF9z2R0OYJaA9XpDvw58DSHU4JhgOug0y5+QXB6fIsLxVVOOZu4wiTSGk9DV5ts+XncLZPsfU1620f3+6AG+wz4NIdHgjQwCnlEOkU0XqLyah6vLGie8XmtcI/T7gZjsgtY1JTP3zXO89+1RW7dn2RkVZO/18eqdFC+v++hHxoJ0rZgchR/NEv9uEvrD9qcndngtcIaP8hdoywljjCRSB5ENj9auoC8kqO0rRn9YAd9fwvl++hEwhdAGgjDQLgOUSGFN2LhlQXzI7t8vbjCKecTylKSlQ6BjvB0QF1l6DcdyjuaVE0h6i2iVuvA/oThliANzLkZgpkyvZLN1kUDfazDeLHN96eu8BV3nZz06WhFJ+px0y+x5E3zYXOe1F2b4h0Pq+mje4PtiD+PoZYgDINgpkz1TIrehODCt2/yV0d+iiUErjCwhEFLRTQUtJTFLzon+M/KcdYrJcp3Fe7SFtrzUQMeS3wewylBCBASYZkEub3y4xcVM6k640b68TKFpq81G1GWepTmVnuSte0yqupgtxXa8yHw93U7+lkMpQRh28hsBpHNsHvCQrxR51i+ycXs8q+sXQoK/H31ddY7JT7+cJ7J9xV2IyS1UkW1WuhIDXws8XkMpwTTRKRS6EyK3qTmDxdusuhUOGltA+4zazeCEter01R2c5RvQu6dj1DdLuHBhP6ZDJeERzuhqXE6J0fxCgb+VMCcU2Xa2sUVT8qKp0MUigdhgWo9i9p1sLoaBnxA80UYHgnSQGbSCNuicW6Cre96jI/U+OH0LX4/c4u0gIK0AQiJaOuAjtJca84iltPkK4L0g96+zoSel6GRIKTYK0OWRa8seG3hHt8o3eFCaoUjpoOJ8XhtpDUdpelqg5qXxm4InF2N0Rn82cAXYWgkICSYJlgWkSuYT9c45XzCmNED7GeWXvFN/vLuD1jdKaPvZJhcDrHrIUatTaji930qQyNBGBLh2GjXJsjAhewKF906FhbyU7c532l+lcqPZ1m81EI2t6G6i/YDor6378O552FoJGBZ6LSLyrhErqYou2TFk0tZCo1ir9TUggyZbYW8s4H2/b0rjDH+RqF4SxAC6TgI20YvzrDxrRK9CU3h1SozZhN4IsHTAcuBYCMqcKs+gdFX6DCEA+4BnoeYS5CIVAqRTtGZyxF8s8l3Fm9zLrPKtPHsJzvQihv+LFc7c2zWCsz2Ioiivd1QjLMAYi5BGAaimCcqZ+mNGIzmOiy4O0yaDaQQwF4/4OmQzUhwuT3PL6tz+FUX2ff2eoIY7oY+TawlyEKO6jem2D0l8GcC/nzmOm9lblOUIa5wCIm4FcANb47LnQV++rPzjF6F+VqEtfqAcB+uq7wIYi1BpFI0FyXZszscK1V5M3ObL1kWYAEQ6IiNsMjlzgKXKnOMXoXSTz6CKCL0vNiXoUfE800daSAsG+1YRI4m73oUrD42Tz7VCoWnQ5a8KT6szLG5XcRuqyfPgSEilpkgXQeRTqEKaYKC4pXCNkdTO+RkAJgoFIGOaGnFP21+hd3/mqRQg/RaA/WoBA1JFkAcJYiH4wnXRTkm2lGMWB0KRvdx2kZaE2hFR0kqrQzpTzTurkK2ukQxbMY+j/hIkMZeBtgW4ZlF6sdT9MYE0/NbvJpep2y0scReP/C+l+Efds9xv1sk+N88E/cCrKaPaA/uvbJBEhsJ0nWQ+Rw6m+bBhTTqzTqT+RY/PPJzfid9/+EqQVdH/KR2nnfe+xrujmT6WoD7yxV0r090ADclXgSxkYBhgGOjXYcgB6+Uqyxkq8xaVQrSJdARDeXT17DVz+FUJamKxq75qEZzrzseUmIjQY6U6J4cxyua9BZ83h67yaxVZdZoAykayueyP8pWUOT6xjTjtyPSm32szRrhEIwmfhOxkaCKWRpHLfplwcJche9lb1GWNpZIAdDSgivdBe50x1DrGfLXtlCr63sChmgn9FkcrARpIFMuwjLxyin6ZYE3ophIt8gIiSUMutrH04r1MM/15gyrjTJ2QyD6/lCXoKc5UAlGqUB4cha/aFM5azHxrQ1eKWzzu8UbOMKkq30ueVnu+BP8c+UMN392guy6ZnLVQzUG+x7ZfnKgEkQqRedIiu6opHvc58/m/4OzziZlCZZwaauAO/4EV9tz3PhkiokrIbnr23svcBzwrbkXycFIkAZCCnQuTXtK0pvSFEfajBht0kJjib3z4r7W3OxOc2l7lmAnhdUKwfPBD9AxPKb8ouy/BCEeN2X9mTyd13t889gy53L3OGk1KEsb4+GYuhLZ/MvKl5BXcpQfaJx7u0Q71b2DmiHsjH8dB5IJwjTBsgmzBkfGqvxe6X9YsHYoSvNxFgD0tUm/4TC6pUlXIkSrg/a8gwh5oOy7BGHbMDlGOJKlPWXw5VyNBWuHstFHPhxRP4OhURYoS+zdQT2E7LsE6Tj0F0o05y1ai3Ahv8aX7QCJ9UwWABhopKmIHEFki72u+hCy/+VICiJbEqYEkRuRM3o44jMyAJBCIQyNNkBLDm0mHMihjtAaoUAoQaTl46sqT/Pod6mUj1cEPyfAik2D/0I5mJM19eQn0MbDXz17ahZpjYEmn+rjFxV+TqDNpBy9EHSkMPsRdtvE7AkaUZqW8jEQSCFQWlNRmkqU4oZ3hJ1GFqspsDoaER6ebenT7L+EXg/n4wfYDzLACL+oHeVrqVUMoTDQ9LXF325f5IO1eYKGw8glg9JSH7PZR+/U9jvcfWH/JYQh4f0NAHIT51hvFlgZG0cKjS1CWlGKD9fncK5kyO9qxj7YRX20hBrySelv4kCfdGbTo75c5q+j38aQCiE0QWSg72ZIb2nslkK2+4daABywBHnnPif+5ghR1n2yRVBgNnaRjQ74waGalv46DlRCVG/AlcavbNEebZ5eFuJ5+eslI5EQAxIJMSCREAMSCTEgkRADEgkxIJEQAxIJMUBofcgHM0NAkgkxIJEQAxIJMSCREAMSCTEgkRADEgkxIJEQAxIJMeD/AFF48fdLhE95AAAAAElFTkSuQmCC"},"metadata":{}}]},{"cell_type":"markdown","source":"Next step is to create a dataset that could be used in a dataloader. A dataset should contain both the features and labels combined for the model to train. Using a dataloader also helps with batching.\nThis step is done for both the train and validation data.","metadata":{}},{"cell_type":"code","source":"dset = list(zip(train_x,train_y))\nx, y = dset[0]\nx.shape, y","metadata":{"execution":{"iopub.status.busy":"2024-01-01T22:36:08.817789Z","iopub.execute_input":"2024-01-01T22:36:08.818259Z","iopub.status.idle":"2024-01-01T22:36:09.334801Z","shell.execute_reply.started":"2024-01-01T22:36:08.818220Z","shell.execute_reply":"2024-01-01T22:36:09.333552Z"},"trusted":true},"execution_count":5,"outputs":[{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"(torch.Size([784]), tensor([1]))"},"metadata":{}}]},{"cell_type":"markdown","source":"First, we wouldnt be using all the 42000 rows of the training data in every run through the model. Hence we use the concept of batching via DataLoaders","metadata":{}},{"cell_type":"code","source":"dl = DataLoader(dset, batch_size=256)","metadata":{"execution":{"iopub.status.busy":"2024-01-01T22:36:20.584784Z","iopub.execute_input":"2024-01-01T22:36:20.586149Z","iopub.status.idle":"2024-01-01T22:36:20.592413Z","shell.execute_reply.started":"2024-01-01T22:36:20.586095Z","shell.execute_reply":"2024-01-01T22:36:20.591090Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":"### Step 1: Initialize the parameters\n\nFirst, we initialize the parameters to random values, and tell PyTorch that we want to track their gradients, using requires_grad_","metadata":{}},{"cell_type":"code","source":"import torch.nn as nn \n\nsimple_net = nn.Sequential(\n    nn.Linear(28*28,30),\n    nn.ReLU(),\n    nn.Linear(30,10)\n)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Step 2: Calculate the predictions\n\n","metadata":{}},{"cell_type":"code","source":"#    for xb,yb in dl:\n#        preds = model(xb)        ","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Step 3: Calculate the loss\n","metadata":{}},{"cell_type":"code","source":"#   loss = mnist_loss(preds, yb.squeeze())","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def mnist_loss(predictions, targets):\n    error = nn.CrossEntropyLoss()\n    return error(predictions, targets)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Step 4: Calculate the gradients\n","metadata":{}},{"cell_type":"code","source":"#        loss.backward()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Step 5: Step the weights.\n","metadata":{}},{"cell_type":"code","source":"#        opt.step()\n#        opt.zero_grad()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Step 6: Repeat the process\n\nEach of the 5 steps above constitute an epoch. The model will be trained over multiple epochs and is designed to get better over each iteration","metadata":{}},{"cell_type":"code","source":"\nopt = torch.optim.Adam(simple_net.parameters(), lr=1e-2)\n\ndef train_epoch(model):\n    total_loss = 0\n    total_correct = 0\n    total = 0\n\n    for xb,yb in dl:\n        preds = model(xb)        \n        loss = mnist_loss(preds, yb.squeeze())\n        loss.backward()\n\n        opt.step()\n        opt.zero_grad()\n        total_loss += loss.item()\n    \n        # Calculate accuracy components\n        with torch.no_grad():  # No need to track gradients here\n            probs = model(xb).softmax(dim=1)\n            preds = torch.max(probs, 1)[1]\n            total_correct += (preds == yb.squeeze()).float().sum().item()\n            total += len(yb)\n\n    avg_loss = total_loss / len(dl)\n    accuracy = 100 * total_correct / total\n    print(f\"Average Loss: {avg_loss}\")\n    print(f\"Accuracy: {accuracy}%\")\n    \n    \ndef train_model(model, epochs):\n    for i in range(epochs):\n        train_epoch(model)\n\ntrain_model(simple_net, 30)","metadata":{"execution":{"iopub.status.busy":"2024-01-01T22:37:21.216067Z","iopub.execute_input":"2024-01-01T22:37:21.216523Z","iopub.status.idle":"2024-01-01T22:37:41.091578Z","shell.execute_reply.started":"2024-01-01T22:37:21.216481Z","shell.execute_reply":"2024-01-01T22:37:41.090696Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"Average Loss: 0.3981167221159646\nAccuracy: 89.26428571428572%\nAverage Loss: 0.1970618763204777\nAccuracy: 94.61428571428571%\nAverage Loss: 0.15053197966436996\nAccuracy: 95.95238095238095%\nAverage Loss: 0.124915442519793\nAccuracy: 96.79523809523809%\nAverage Loss: 0.10788022199165866\nAccuracy: 97.22619047619048%\nAverage Loss: 0.0948949148666791\nAccuracy: 97.60238095238095%\nAverage Loss: 0.08345539275077031\nAccuracy: 97.93809523809524%\nAverage Loss: 0.07362912242780581\nAccuracy: 98.27142857142857%\nAverage Loss: 0.06961617553394965\nAccuracy: 98.31190476190476%\nAverage Loss: 0.06444140522948918\nAccuracy: 98.51666666666667%\nAverage Loss: 0.06153505816191407\nAccuracy: 98.63809523809523%\nAverage Loss: 0.06125885867989961\nAccuracy: 98.60952380952381%\nAverage Loss: 0.0642240517359107\nAccuracy: 98.56666666666666%\nAverage Loss: 0.05708026345589932\nAccuracy: 98.69047619047619%\nAverage Loss: 0.05598650830480355\nAccuracy: 98.7547619047619%\nAverage Loss: 0.05579148306128583\nAccuracy: 98.8%\nAverage Loss: 0.0509220442649993\nAccuracy: 98.8952380952381%\nAverage Loss: 0.04484355623523394\nAccuracy: 99.05238095238096%\nAverage Loss: 0.04118587809145648\nAccuracy: 99.15714285714286%\nAverage Loss: 0.042009486610098096\nAccuracy: 99.14047619047619%\nAverage Loss: 0.039592770768054335\nAccuracy: 99.1452380952381%\nAverage Loss: 0.040688764031493165\nAccuracy: 99.14761904761905%\nAverage Loss: 0.03265123971209643\nAccuracy: 99.34285714285714%\nAverage Loss: 0.03305316193026639\nAccuracy: 99.39761904761905%\nAverage Loss: 0.032050055304110474\nAccuracy: 99.3952380952381%\nAverage Loss: 0.038136395928590486\nAccuracy: 99.27380952380952%\nAverage Loss: 0.03732619106051447\nAccuracy: 99.32857142857142%\nAverage Loss: 0.029490354775093278\nAccuracy: 99.47619047619048%\nAverage Loss: 0.029981625288520733\nAccuracy: 99.49285714285715%\nAverage Loss: 0.029507957270481822\nAccuracy: 99.52142857142857%\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Never go full scratch! \n\nThe above code had some inbuilt Pytorch functions that took care of parameter initialization, loss function and optimization function for the gradients. CrossEntropy loss wasnt even covered in Chapter4!\n\nHowever, in the spirit of developing a model from scratch, I have replaced the built-in functions with steps that explain how each of the actions are performed. The performance takes a hit compared to the above implementation, but is still a good experience.","metadata":{"execution":{"iopub.status.busy":"2024-01-01T18:27:30.744971Z","iopub.execute_input":"2024-01-01T18:27:30.746173Z","iopub.status.idle":"2024-01-01T18:27:30.752447Z","shell.execute_reply.started":"2024-01-01T18:27:30.746081Z","shell.execute_reply":"2024-01-01T18:27:30.751008Z"}}},{"cell_type":"markdown","source":"### Optimizer function\nclass BasicOptim performs the same initialization, stepping of weights and resetting bias to 0, which was done by the Adam Optimizer earlier.\n","metadata":{}},{"cell_type":"code","source":"class basicOptim:\n    def __init__(self, params, lr): self.params, self.lr = list(params), lr\n    \n    def step(self, *args, **kwargs):\n        for p in self.params: p.data -= p.grad.data * self.lr \n    \n    def zero_grad(self, *args, **kwargs):\n        for p in self.params: p.grad = None","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Loss function\nClass BasicOptim performs the same initialization, stepping of weights and resetting bias to 0, which was done by the Adam Optimizer earlier.","metadata":{}},{"cell_type":"code","source":"def softmax(x):\n    # Subtract max value for numerical stability\n    exp_x = torch.exp(x - torch.max(x, dim=1, keepdim=True)[0])\n    return exp_x / torch.sum(exp_x, dim=1, keepdim=True)\n\n\ndef cross_entropy_loss(predictions, targets):\n    \n    # Step 1: Apply softmax to convert logits to probabilities\n    probs = softmax(predictions)\n\n    # Step 2: Gather the probabilities of the true classes for each example.\n    batch_size = predictions.shape[0]\n    correct_class_probs = probs[range(batch_size), targets]\n\n    # Step 3: Compute the negative log likelihood\n    epsilon = 1e-12\n    loss = -torch.log(correct_class_probs + epsilon)\n\n    # Step 4: Take the average loss over the batch\n    return torch.mean(loss)\n\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Parameter Initialization\nAll we have in simple_net is two linear classifiers with a max function between them. That little function res.max(tensor(0.0)) is called a rectified linear unit, also known as ReLU\n\nHere, w1 and w2 are weight tensors, and b1 and b2 are bias tensors; that is, parameters that are initially randomly initialized.","metadata":{}},{"cell_type":"code","source":"def init_params(size, std=1.0): return (torch.randn(size)*std).requires_grad_()\n\nw1 = init_params((28*28,30))\nb1 = init_params(30)\nw2 = init_params((30,10))\nb2 = init_params(10)\n\n\ndef simple_net(xb):\n    res = xb @ w1 + b1\n    res = res.max(tensor(0.0))\n    res = res @ w2 + b2\n    return res","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch.nn as nn \nimport torch\n\ndef softmax(x):\n    # Subtract max value for numerical stability\n    exp_x = torch.exp(x - torch.max(x, dim=1, keepdim=True)[0])\n    return exp_x / torch.sum(exp_x, dim=1, keepdim=True)\n\n\ndef cross_entropy_loss(predictions, targets):\n    \n    # Step 1: Apply softmax to convert logits to probabilities\n    probs = softmax(predictions)\n\n    # Step 2: Gather the probabilities of the true classes for each example.\n    batch_size = predictions.shape[0]\n    correct_class_probs = probs[range(batch_size), targets]\n\n    # Step 3: Compute the negative log likelihood\n    epsilon = 1e-12\n    loss = -torch.log(correct_class_probs + epsilon)\n\n    # Step 4: Take the average loss over the batch\n    return torch.mean(loss)\n\n\n     \nclass basicOptim:\n    def __init__(self, params, lr): self.params, self.lr = list(params), lr\n    \n    def step(self, *args, **kwargs):\n        for p in self.params: p.data -= p.grad.data * self.lr \n    \n    def zero_grad(self, *args, **kwargs):\n        for p in self.params: p.grad = None\n\ndef init_params(size, std=1.0):\n    return (torch.randn(size)*std).requires_grad_()\n    \n            \ndef simple_net(xb):\n    \n    res = xb @ w1 + b1\n    res = res.max(tensor(0.0))\n    res = res @ w2 + b2\n    \n    return res\n            \n\nw1 = init_params((28*28,30))\nb1 = init_params(30)\nw2 = init_params((30,10))\nb2 = init_params(10)\nlr = 1e-2\nopt = basicOptim([w1,b1,w2,b2], lr)\n\n\n    \ndef train_epoch(model):\n    total_loss = 0\n    total_correct = 0\n    total = 0\n\n    for xb,yb in dl:\n        preds = model(xb)        \n        loss = cross_entropy_loss(preds, yb.squeeze())\n        loss.backward()\n\n        opt.step()\n        opt.zero_grad()\n        total_loss += loss.item()\n    \n        # Calculate accuracy components\n        with torch.no_grad():  # No need to track gradients here\n            probs = model(xb).softmax(dim=1)\n            preds = torch.max(probs, 1)[1]\n            total_correct += (preds == yb.squeeze()).float().sum().item()\n            total += len(yb)\n\n    avg_loss = total_loss / len(dl)\n    accuracy = 100 * total_correct / total\n    print(f\"Average Loss: {avg_loss}\")\n    print(f\"Accuracy: {accuracy}%\")\n    \n\n    \ndef train_model(model, epochs):\n    for i in range(epochs):\n        train_epoch(model)\n\ntrain_model(simple_net, 30)","metadata":{"execution":{"iopub.status.busy":"2024-01-01T23:30:44.150887Z","iopub.execute_input":"2024-01-01T23:30:44.151394Z","iopub.status.idle":"2024-01-01T23:30:59.422856Z","shell.execute_reply.started":"2024-01-01T23:30:44.151354Z","shell.execute_reply":"2024-01-01T23:30:59.421636Z"},"trusted":true},"execution_count":16,"outputs":[{"name":"stdout","text":"Average Loss: 19.682425007675633\nAccuracy: 20.154761904761905%\nAverage Loss: 15.231511861627752\nAccuracy: 34.75%\nAverage Loss: 13.083408321033824\nAccuracy: 42.76904761904762%\nAverage Loss: 11.632037850582238\nAccuracy: 47.09761904761905%\nAverage Loss: 9.626058815464829\nAccuracy: 50.23571428571429%\nAverage Loss: 7.138738707340125\nAccuracy: 54.67857142857143%\nAverage Loss: 5.555344997030316\nAccuracy: 59.61666666666667%\nAverage Loss: 4.529182726325411\nAccuracy: 63.195238095238096%\nAverage Loss: 3.76052830472137\nAccuracy: 65.73571428571428%\nAverage Loss: 3.1792309446768328\nAccuracy: 67.53333333333333%\nAverage Loss: 2.751117691487977\nAccuracy: 68.98571428571428%\nAverage Loss: 2.4279137311559733\nAccuracy: 70.03095238095239%\nAverage Loss: 2.1788007548361112\nAccuracy: 70.9047619047619%\nAverage Loss: 1.9816493820060384\nAccuracy: 71.64761904761905%\nAverage Loss: 1.82115953745264\nAccuracy: 72.17619047619047%\nAverage Loss: 1.6878934966795371\nAccuracy: 72.68095238095238%\nAverage Loss: 1.5753142022725308\nAccuracy: 73.11428571428571%\nAverage Loss: 1.47932684024175\nAccuracy: 73.35952380952381%\nAverage Loss: 1.3963795774813854\nAccuracy: 73.61904761904762%\nAverage Loss: 1.3245267420104057\nAccuracy: 73.8547619047619%\nAverage Loss: 1.2619493107904087\nAccuracy: 74.10714285714286%\nAverage Loss: 1.2072875248663353\nAccuracy: 74.35%\nAverage Loss: 1.159389293103507\nAccuracy: 74.53095238095239%\nAverage Loss: 1.117131485993212\nAccuracy: 74.76666666666667%\nAverage Loss: 1.0796435102368847\nAccuracy: 74.9952380952381%\nAverage Loss: 1.046269206567244\nAccuracy: 75.17380952380952%\nAverage Loss: 1.0163917527957396\nAccuracy: 75.29523809523809%\nAverage Loss: 0.9895745481505539\nAccuracy: 75.51666666666667%\nAverage Loss: 0.9652250724308419\nAccuracy: 75.69047619047619%\nAverage Loss: 0.9430474978504759\nAccuracy: 75.87142857142857%\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### Conclusion \n\nIn this notebook, I have trained a neural network to perform multi-class classification on the MNIST dataset and achieved reasonable results. I have also replaced the built-in pyTorch functions with ones that recreate the same steps in training a model.","metadata":{}}]}